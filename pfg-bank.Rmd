---
title: "PFG-bank: Data Driven Credit Card Design"
output: 
  html_document:
    toc: True
    
---

* Team-lead gitlab id:2724742
* Group number:10
* Group name: Group_10
* Team member names: Menghui Zhang, Shumeng Shi, Wenrui Li

```{r r_setup, include = FALSE}
## initial settings
knitr::opts_chunk$set(
  comment = NA,
  echo = TRUE,
  error = TRUE,
  cache = FALSE,
  message = FALSE,
  dpi = 96,
  warning = FALSE
)

## width to use when printing tables etc.
options(
  width = 250,
  scipen = 100,
  max.print = 5000,
  stringsAsFactors = FALSE
)
```

<style>
.table {
  width: auto;
}
ul, ol {
  padding-left: 18px;
}
pre, code, pre code {
  overflow: auto;
  white-space: pre;
  word-wrap: normal;
  background-color: #ffffff;
}
</style>

## Setup

Please complete this R-markdown document by answering the questions in `pfg-bank.pdf` on Dropbox (week10/readings/). The code block below will load the historical data from exhibits 1 and 2. Please DO NOT change the code used to load the data. Create an HTML (Notebook) file with all your results and comments and push both the Rmarkdown and HTML file to GitLab when you are done. All analysis results MUST be reproducible (i.e., the TA and I must be able to recreate the HTML from the R-markdown file without changes or errors).

```{r}
fp <- radiant.data::find_dropbox()
exhibit1 <- readxl::read_excel(file.path(fp, "MGTA455-2019/data/exhibits.xlsx"), sheet = "exhibit1")
exhibit2 <- readxl::read_excel(file.path(fp, "MGTA455-2019/data/exhibits.xlsx"), sheet = "exhibit2")
```

## Question answers

### Question 1

```{r include = FALSE}
library(dplyr)
library(tidyverse)
library(radiant)
library(ggplot2)
library(data.table)
library(stringr)
```

As we can see in exhibit2, the expected LTVs vary with BK score groups. The higher the BK score, the lower the lifetime customer value. It makes sense as the probability of defaulting rises with bankcruptcy score and we adjust downwards our expected repayments. From BK150 to BK200, LTV decreases by \$20; and from BK200 to BK250, it decreases by \$30.

```{r}
exhibit2 %>% 
  select(ltv150, ltv200, ltv250) %>%
  colMeans()

```

In addition, within each BK group, we can see LTV varies with product features. 
 
* 1. LTV increases with APR. With higher interest rate charges, PFG has higher expected revenues, hence higher projected LTV. From APR 14.9 to APR 16.8, LTV increases by \$20; and from APR 16.8 to 19.8, LTV increases by \$28.

```{r}
# ARP
exhibit2 %>% 
  group_by(apr) %>% 
  summarise(avg_bk150 = mean(ltv150),
            avg_bk200 = mean(ltv200),
            avg_bk250 = mean(ltv250))
```

* 2. LTV will also rise if banks issue credit cars with variable rates, because banks have the channels to pass along rising borrowing costs to customers, hence protecting their margins. This protection is reflected in $10 increase in customer LTV. 

```{r}
# Fixed vs Var

exhibit2 %>% 
  group_by(fixed_var) %>% 
  summarise(avg_bk150 = mean(ltv150),
            avg_bk200 = mean(ltv200),
            avg_bk250 = mean(ltv250))
```

* 3. Higher annual fee also means higher projected LTV, as the fixed annual fee of \$20 is guranteed revenues for banks. Translated to customer LTV, it's equivalent to \$31.


```{r}
# Annual Fee

exhibit2 %>% 
  group_by(annual_fee) %>% 
  summarise(avg_bk150 = mean(ltv150),
            avg_bk200 = mean(ltv200),
            avg_bk250 = mean(ltv250))
```


### Question 2

In this question, we separated exhibit1 dataset by three bankruptcy score groups and expanded datasets to the length of mailing counts. Then we ran logistic regressions using the actual response as the predicted variable, and APR rate, fixed/variable rate and annual fee as the explanatory variables. Below are the predictive results. 

```{r}
# expand data frame

exhibit1_150 <- exhibit1 %>%
  filter(bk_score == 150) %>% 
  select(apr, fixed_var, annual_fee, non_resp, resp) %>% 
  gather(key = 'result', value = 'count', -c(apr, fixed_var, annual_fee)) %>% 
  slice(rep(1:n(), count)) %>% 
  select(-count)


exhibit1_200 <- exhibit1 %>%
  filter(bk_score == 200) %>% 
  select(apr, fixed_var, annual_fee, non_resp, resp) %>% 
  gather(key = 'result', value = 'count', -c(apr, fixed_var, annual_fee)) %>% 
  slice(rep(1:n(), count)) %>% 
  select(-count)

exhibit1_250 <- exhibit1 %>%
  filter(bk_score == 250) %>% 
  select(apr, fixed_var, annual_fee, non_resp, resp) %>% 
  gather(key = 'result', value = 'count', -c(apr, fixed_var, annual_fee)) %>% 
  slice(rep(1:n(), count)) %>% 
  select(-count)


exhibit1_150 <- exhibit1_150 %>% mutate_all(as.factor)
exhibit1_200 <- exhibit1_200 %>% mutate_all(as.factor)
exhibit1_250 <- exhibit1_250 %>% mutate_all(as.factor)

```


**BK150 Group**

```{r fig.width = 7, fig.height = 4.09, dpi = 144}
result150 <- logistic(
  exhibit1_150, 
  rvar = "result", 
  evar = c("apr", "fixed_var", "annual_fee"), 
  lev = "resp"
)
summary(result150)
```

**BK200 Group**

```{r fig.width = 7, fig.height = 3.88, dpi = 144}
result200 <- logistic(
  exhibit1_200, 
  rvar = "result", 
  evar = c("apr", "annual_fee"), 
  lev = "resp"
)
summary(result200)
```


**BK250 Group**

```{r fig.width = 7, fig.height = 4.31, dpi = 144}
result250 <- logistic(
  exhibit1_250, 
  rvar = "result", 
  evar = c("apr", "fixed_var", "annual_fee"), 
  lev = "resp"
)
summary(result250)
```

Generally, we don't find the predictive models useful in picking up product features' effects on response rate across BK groups and here are the reasons. 

* 1. The product feature combinations aren't comprehensive in BK150 and BK200, which makes it difficult to compare the effects of one feature while holding other features the same. To be specific, In group BK150, we don't have an instance of APR 19.8; while in group BK200, we don't have APR 16.8 or fixed/variable rate.

* 2. External factors have settled in since the launch of previous mailings, in terms of competitors' offers and prevailing borrowing costs. Both conditions affect how customers would react to PFG's product combinations. 

However, the historical data did affirm our assumptions on how variations in these variables would affect customers' response rate. For example, in group BK250, the odds ratio of annual_fee|20 is 0.284, meaning that if PFG charges $20 for the annual fee, the odds of a customer opening an account will decrease by 71.6%, holding all else constant. Also, given the current variables we have, we see that different BK groups react to the same product differently. Below is odds ratio comparison among different BK groups.


```{r}
# compare product feature effect across different BK groups

coeffs <- c("apr|16.8", "apr|19.8", "fixed_var|Variable", "annual_fee|20")

# 250, 200, 150
apr_168 <- c(result250[["coeff"]][["OR"]][2], 0, result150[["coeff"]][["OR"]][2])
apr_198 <- c(result250[["coeff"]][["OR"]][3], result200[["coeff"]][["OR"]][2],0)
var_to_fixed <- c(result250[["coeff"]][["OR"]][4], 0, result150[["coeff"]][["OR"]][3])
annual_fee <- c(result250[["coeff"]][["OR"]][5], result200[["coeff"]][["OR"]][3], result150[["coeff"]][["OR"]][4])

or_df <- data.frame(bkgroup = c("BK250", "BK200", "BK150"),
                    apr_168 = apr_168,
                    apr_198 = apr_198,
                    var_to_fixed = var_to_fixed,
                    annual_fee = annual_fee)

or_df %>% 
  gather(key = 'feature', value = 'odds_ratio', -bkgroup) %>% 
  ggplot(aes(x = factor(bkgroup), y=odds_ratio)) +
  geom_bar(stat = 'identity', position = "dodge", aes(fill = feature)) +
  scale_fill_brewer(palette = "Set1") + 
  labs(main = "Product Feature Effects on Response Odds Ratio", 
       x = 'BK Group')

```

### Question 3

We found the most 'preferable' offer after round 1 test. After getting the test results, we ran logistic regressions for each BK group, like what we did for historical data and predicted response probabilities afterwards. 

For each group of customers, we extracted the offer with highest predicted response rate. It turns out that all 3 groups prefer 14.9 ARP rate, fixed-rate APR and zero annual fees. It makes sense as customers want the products that are the most cost-effective. 

```{r message = FALSE}
# import round 1 test results

pfg_150 <- read_csv("pfg_150.csv")
pfg_200 <- read_csv("pfg_200.csv")
pfg_250 <- read_csv("pfg_250.csv")

# change type product and expand for logistic regression

pfg_150 <- mutate(pfg_150, no_resp = Sent - Responses)
pfg_200 <- mutate(pfg_200, no_resp = Sent - Responses)
pfg_250 <- mutate(pfg_250, no_resp = Sent - Responses)


pfg_150 <- gather(pfg_150, responce, freq, Responses, no_resp, factor_key = TRUE)
pfg_200 <- gather(pfg_200, responce, freq, Responses, no_resp, factor_key = TRUE)
pfg_250 <- gather(pfg_250, responce, freq, Responses, no_resp, factor_key = TRUE)

pfg_150 <- mutate_at(pfg_150, .vars = vars(apr, annual_fee), .funs = as_factor)
pfg_200 <- mutate_at(pfg_200, .vars = vars(apr, annual_fee), .funs = as_factor)
pfg_250 <- mutate_at(pfg_250, .vars = vars(apr, annual_fee), .funs = as_factor)

# input for response rate prediction

exhibit2 <- mutate_at(exhibit2, .vars = vars(apr, annual_fee), .funs = as_factor)

## expanded data

pfg_150_dat <- select(pfg_150, profile, apr, fixed_var, annual_fee, Sent, responce, freq) %>%
  table2data("freq")

pfg_200_dat <- select(pfg_200, profile, apr, fixed_var, annual_fee, Sent, responce, freq) %>%
  table2data("freq")

pfg_250_dat <- select(pfg_250, profile, apr, fixed_var, annual_fee, Sent, responce, freq) %>%
  table2data("freq")


```


**Logistic Regression of BK150 after Round1**

```{r}
result <- logistic(
  pfg_150, 
  rvar = "responce", 
  evar = c("apr", "fixed_var", "annual_fee"), 
  lev = "Responses", 
  wts = "freq"
)
summary(result)


pred <- predict(result, pred_data = exhibit2)
print(pred)
exhibit2 <- store(exhibit2, pred, name = "pred_150")
exhibit2$pred_150_lower <- pred$`2.5%`
exhibit2$pred_150_higher <- pred$`97.5%`

pred <- predict(result, pred_data = pfg_150_dat)
pfg_150_dat <- store(pfg_150_dat, pred, name = "pred_150")
```


**Logistic Regression of BK200 after Round1**

```{r}

result <- logistic(
  pfg_200, 
  rvar = "responce", 
  evar = c("apr", "fixed_var", "annual_fee"), 
  lev = "Responses", 
  wts = "freq"
)
summary(result)

pred <- predict(result, pred_data = exhibit2)
print(pred)
exhibit2 <- store(exhibit2, pred, name = "pred_200")
exhibit2$pred_200_lower <- pred$`2.5%`
exhibit2$pred_200_higher <- pred$`97.5%`


pred <- predict(result, pred_data = pfg_200_dat)
pfg_200_dat <- store(pfg_200_dat, pred, name = "pred_200")
```

**Logistic Regression of BK250 after Round1**


```{r}

result <- logistic(
  pfg_250, 
  rvar = "responce", 
  evar = c("apr", "fixed_var", "annual_fee"), 
  lev = "Responses", 
  wts = "freq"
)
summary(result)

pred <- predict(result, pred_data = exhibit2)
print(pred, n = 10)
exhibit2 <- store(exhibit2, pred, name = "pred_250")
exhibit2$pred_250_lower <- pred$`2.5%`
exhibit2$pred_250_higher <- pred$`97.5%`

pred <- predict(result, pred_data = pfg_250_dat)
pfg_250_dat <- store(pfg_250_dat, pred, name = "pred_250")
```


```{r}
exhibit2[which.max(exhibit2$pred_150),]
exhibit2[which.max(exhibit2$pred_200),]
exhibit2[which.max(exhibit2$pred_250),]

```

### Question 4

#### Before Round 1

The decision of test setting consists of two steps. The first is which combinations to test, the second is test size for each cell.

**Partial or full factorial design?**

In this case, we think that full factorial testing will burn too many customers who could have been targeted by specific credit plans instead. We believe that after careful subset, we will be able to be exposed to necessary information about the most important features.

**Test combination**

We use Radiant DOB function to help us decide which combinations to test. The design factors are 'apr', 'annual fee' and 'fixed_var'. The reason we did not include br_score is that bk_score is customer attribute rather than product attribute and we can not control. It is necessary that each customer type was exposed to the same kinds of combinations, which will help PFG to gain a full picture in the customer response pattern. In DOB, design efficiency has first exceeded 0.8 at the 6th trials and the set is balanced. Thus, we decided to test 6 combinations for each customer type.


The 6 combinations we chose are as follows:

```{r}
fac_result <- doe(
  factors = c(
    "apr; 14.9; 16.8; 19.8", 
    "annual_fee; 0; 20", 
    "fixed_var; 'fixed'; 'var'"
  ), 
  trials = 6, 
  seed = 1234
)
summary(fac_result, eff = FALSE, part = TRUE, full = FALSE)
```

With different random seeds, factorial designs are different mainly in terms of annual fee and fixed_var. Under each instance of apr, the test settings are either \$0_variable and \$20_fixed or \$0_fixed and \$20_variable. Between those two we prefer \$0_variable and \$20_fixed. What we have affirmed from historical data is that higher annual fee is associated with a lower response rate, and customers have a more positive attitude towards fixed rate compared to variable rate feature. Thus the combination of $0 and variable would drive customers' decisions in opposite directions. Therefore we expect tests on these combinations would reveal more unseen information on customer behaviors. 

However, we also need combinations of zero annual fee and fixed rate offer to our test balanced, so we combined zero annual_fee and fixed rate under APR of 19.8. 

**Sample size**

After determining the product combinations, we moved on to choosing sample size. 

First, through logistic regressions on historical data, we found that the approximate sample size would be around 18,000 for each kind of customers to draw statistical conclusions. 

```{r}

# take PKG250 historical data for example

exhibit1_150 <- exhibit1 %>%
  filter(bk_score == 150) %>% 
  select(apr, fixed_var, annual_fee, non_resp, resp) %>% 
  gather(key = 'result', value = 'count', -c(apr, fixed_var, annual_fee)) %>% 
  slice(rep(1:n(), count)) %>% 
  select(-count)

# randomly chose 18,000 rows to run regression and see whether the coefficients are statistical significant 

test <-  exhibit1_250[sample(nrow(exhibit1_250), 18000), ]

# ensure the average response rate of test set is  similar to that of original dataset
mean(exhibit1_250$result == "resp")
mean(test$result == "resp")

test250 <- logistic(
  test, 
  rvar = "result", 
  evar = c("apr", "fixed_var", "annual_fee"), 
  lev = "resp"
)
summary(test250)


```

Next, we decide the sample size to be allocated to each testing offer. Instead of evenly allocating 18,000 targets to 6 combinations, we decide to differentiate sample size according to the uncertainty and ambiguity of test results. We use historical average response rate for reference. 

```{r}
avg_resp <- exhibit1 %>% 
  mutate(combo = paste(apr, fixed_var, annual_fee)) %>% 
  group_by(combo) %>% 
  summarise(mailed = sum(nr_mailed), resp = sum(resp), perc = resp/mailed) %>% 
  arrange(desc(perc))

avg_resp
```

For example, for those combinations at the spectrum of all test settings, we may allocate a smaller sample size as we are relatively sure their response rate would be at top and bottom. For those combinations at the middle, response difference between each case is relative small, thus large sample size is needed to make the result statistically significant. Sample size we choose for 6 test combinations are 2000,3000,4000, 4000,3000,2000 respectively.

```{r}
fac_part <- fac_result[["part"]]
fac_part$past_resp <- c('<0.0490', '0.0235', '<0.0104', '0.0104','0.0214', '<0.0043')
fac_part$sample_size <- c(2000,3000,4000,4000,3000,2000)
fac_part
```


#### After Round 1

##### Recommendation for Each Group

After putting our decisions to test, we gathered the results and used logistic regression for our analysis. Model details are illustrated in Question3. 

In our final rolling out, we chose our offer that has the highest expected CLV, which is the corresponding CLV multiplied by predicted response rate. 

* Group BK150 - Offer 4, Apr 14.9, Variable Rate, Zero Annual Fee;

* Group BK200 - Offer 3, APR 14.9, Fixed Rate, Zero Annual Fee;

* Group BK250 - Offer 11, APR 19.8, Fixed Rate, Zero Annual Fee. 


```{r}
# calculated expected CLV for each group 
exhibit2 <- exhibit2 %>%
  mutate(profit_150 = ltv150*pred_150,
         profit_200 = ltv200*pred_200,
         profit_250 = ltv250*pred_250)

# take a second look at upper and lower bound of expected CLV
exhibit2 <- exhibit2 %>%
  mutate(profit_150_lower = ltv150*pred_150_lower,
         profit_150_higher = ltv150*pred_150_higher,
         profit_200_lower = ltv200*pred_200_lower,
         profit_200_higher = ltv200*pred_200_higher,
         profit_250_lower = ltv250*pred_250_lower,
         profit_250_higher = ltv250*pred_250_higher)

# get offers that have the highest expected CLV
send_150 <- exhibit2 %>%
  arrange(desc(profit_150) )%>%
            slice(1) %>% 
  select(offer, apr, fixed_var, annual_fee, pred_150)
send_150 

send_200 <- exhibit2 %>%
  arrange(desc(profit_200) )%>%
            slice(1) %>% 
  select(offer, apr, fixed_var, annual_fee, pred_200)
send_200



send_250 <- exhibit2 %>%
  arrange(desc(profit_250) )%>%
            slice(1) %>% 
  select(offer, apr, fixed_var, annual_fee, pred_250)

send_250


```

##### Neural Network - Second Model

We also ran neural network report to capture remaining interaction effects if any. 

**Neural Network Model for BK150**

```{r}
result <- nn(
  pfg_150, 
  rvar = "responce", 
  evar = c("apr", "fixed_var", "annual_fee"), 
  lev = "Responses", 
  wts = "freq", 
  seed = 1234
)
pred <- predict(result, pred_data = exhibit2)
print(pred)
exhibit2 <- store(exhibit2, pred, name = "pred_nn_150")

pred <- predict(result, pred_data = pfg_150_dat)
pfg_150_dat <- store(pfg_150_dat, pred, name = "pred_nn_150")
```

**Neural Network Model for BK200**

```{r}
result <- nn(
  pfg_200, 
  rvar = "responce", 
  evar = c("apr", "fixed_var", "annual_fee"), 
  lev = "Responses", 
  wts = "freq", 
  seed = 1234
)
pred <- predict(result, pred_data = exhibit2)
print(pred)
exhibit2 <- store(exhibit2, pred, name = "pred_nn_200")

pred <- predict(result, pred_data = pfg_200_dat)
pfg_200_dat <- store(pfg_200_dat, pred, name = "pred_nn_200")
```

**Neural Network Model for BK250**

```{r}
result <- nn(
  pfg_250, 
  rvar = "responce", 
  evar = c("apr", "fixed_var", "annual_fee"), 
  lev = "Responses", 
  wts = "freq", 
  seed = 1234
)
summary(result, prn = TRUE)
pred <- predict(result, pred_data = exhibit2)
print(pred)
exhibit2 <- store(exhibit2, pred, name = "pred_nn_250")

pred <- predict(result, pred_data = pfg_250_dat)
pfg_250_dat <- store(pfg_250_dat, pred, name = "pred_nn_250")
```


Next we checked the expected CLV ranking as well and below are the recommendations. The results are different from previous recommendations where variable rate plans are recommendated for BK200 and BK250. 

* Group BK150 - Offer 4, Apr 14.9, Variable Rate, Zero Annual Fee;

* Group BK200 - Offer 4, Apr 14.9, Variable Rate, Zero Annual Fee;

* Group BK250 - Offer 12, APR 19.8, Variable Rate, Zero Annual Fee. 

```{r}
# calculate expected CLV
exhibit2 <- exhibit2 %>%
  mutate(profit_nn_150 = ltv150*pred_nn_150,
         profit_nn_200 = ltv200*pred_nn_200,
         profit_nn_250 = ltv250*pred_nn_250)

exhibit2[which.max(exhibit2$profit_nn_150),c('offer','apr','fixed_var', 'annual_fee', 'profit_150')]
exhibit2[which.max(exhibit2$profit_nn_200),c('offer','apr','fixed_var', 'annual_fee', 'profit_200')]
exhibit2[which.max(exhibit2$profit_nn_250),c('offer','apr','fixed_var', 'annual_fee', 'profit_250')]
```

##### Ensemble

We then ensembled the predictions and checked the expected clv. The results are the same with logistic regression. 


```{r}
exhibit2 <- exhibit2 %>%
  mutate(ensemble_150  = (pred_150 +pred_nn_150) / 2,
         ensemble_200  = (pred_200 +pred_nn_200) / 2,
         ensemble_250  = (pred_250 +pred_nn_250) / 2,
         profit_ensem_150 = ensemble_150 * ltv150,
         profit_ensem_200 = ensemble_200 * ltv200,
         profit_ensem_250 = ensemble_250 * ltv250)

exhibit2[which.max(exhibit2$profit_ensem_150),c('offer','apr','fixed_var', 'annual_fee', 'ensemble_150')]
exhibit2[which.max(exhibit2$profit_ensem_200),c('offer','apr','fixed_var', 'annual_fee', 'ensemble_200')]
exhibit2[which.max(exhibit2$profit_ensem_250),c('offer','apr','fixed_var', 'annual_fee', 'ensemble_250')]

## ensemble predict on expanded train set
pfg_150_dat <- pfg_150_dat %>%
  mutate(ensemble_150 =(pred_150 + pred_nn_150)/2 )

pfg_200_dat <- pfg_200_dat %>%
  mutate(ensemble_200 =(pred_200 + pred_nn_200)/2 )

pfg_250_dat <- pfg_250_dat %>%
  mutate(ensemble_250 =(pred_250 + pred_nn_250)/2 )
```

##### Compare AUC

To validate our final choice, we calculate the model aucs on expanded train set. It turns out that logistic is slightly better than the other two. 

```{r}
# define evaluation function
auc <- function(dat, vars){
  
  cm_df <- as.data.frame(matrix(NA, ncol = 2, nrow = length(vars)))
  colnames(cm_df) <- c("var", "auc")
  
  for (i in 1:length(vars)){
    
    var <- vars[i]
    probs <- pull(dat, !!var)
    Resp <- pull(dat, "responce")
    
    #pred <- ifelse(pull(dat, !!var) > 0.5, "yes", "no")
    
    auc <- ModelMetrics::auc(ifelse(Resp=="Responses",1,0), probs)

    cm_vec <- c(var, auc)
    cm_df[i,] <- cm_vec
    
    cm_df[2] <- lapply(cm_df[2], as.numeric)
  }
  return(cm_df)
}


auc(pfg_150_dat,c('pred_150','pred_nn_150','ensemble_150'))
auc(pfg_200_dat,c('pred_200','pred_nn_200','ensemble_200'))
auc(pfg_250_dat,c('pred_250','pred_nn_250','ensemble_250'))
```



For most of the case, logistic regression, neural network and ensemble have equal predictive power in terms of auc. Logistic regression is slightly better. Therefore this is our final choice for each group of customers. 

* Group BK150 - Offer 4, Apr 14.9, Variable Rate, Zero Annual Fee;

* Group BK200 - Offer 3, APR 14.9, Fixed Rate, Zero Annual Fee;

* Group BK250 - Offer 11, APR 19.8, Fixed Rate, Zero Annual Fee. 
